Run these once in SageMaker Studio terminal:
pip install dvc[s3]
dvc init
dvc remote add -d s3remote s3://sarthak-sagemaker-bucket-2025/dvc-store
dvc add housing.csv
git add housing.csv.dvc .dvc/config
git commit -m "Add housing dataset with DVC"

Install required libraries:
pip install mlflow wandb sagemaker

Login to WandB:
wandb login

Code + MLflow + WandB:
import argparse
import os
import joblib
import pandas as pd
import numpy as np

import mlflow
import wandb

from sklearn.model_selection import StratifiedShuffleSplit
from sklearn.preprocessing import OneHotEncoder, StandardScaler
from sklearn.compose import ColumnTransformer
from sklearn.pipeline import Pipeline
from sklearn.ensemble import RandomForestRegressor
from sklearn.impute import SimpleImputer
from sklearn.metrics import mean_squared_error

# -------------------------------
# ARGUMENTS (for SageMaker tuning)
# -------------------------------
parser = argparse.ArgumentParser()
parser.add_argument("--n_estimators", type=int, default=100)
parser.add_argument("--max_depth", type=int, default=None)
parser.add_argument("--min_samples_split", type=int, default=2)
args = parser.parse_args()

# -------------------------------
# INIT TRACKING
# -------------------------------
mlflow.start_run()

wandb.init(
    project="sagemaker-housing",
    config=vars(args)
)

# -------------------------------
# LOAD DATA (DVC-managed file)
# -------------------------------
df = pd.read_csv("housing.csv")

df["income_cat"] = pd.cut(
    df["median_income"],
    bins=[0., 1.5, 3.0, 4.5, 6., np.inf],
    labels=[1, 2, 3, 4, 5]
)

split = StratifiedShuffleSplit(n_splits=1, test_size=0.2, random_state=42)
for train_idx, test_idx in split.split(df, df["income_cat"]):
    train = df.loc[train_idx].drop("income_cat", axis=1)
    test = df.loc[test_idx].drop("income_cat", axis=1)

y_train = train["median_house_value"]
X_train = train.drop("median_house_value", axis=1)

y_test = test["median_house_value"]
X_test = test.drop("median_house_value", axis=1)

cat_attr = ["ocean_proximity"]
num_attr = X_train.drop("ocean_proximity", axis=1).columns.tolist()

# -------------------------------
# PIPELINE
# -------------------------------
num_pipeline = Pipeline([
    ("imputer", SimpleImputer(strategy="median")),
    ("scaler", StandardScaler())
])

cat_pipeline = Pipeline([
    ("onehot", OneHotEncoder(handle_unknown="ignore"))
])

pipeline = ColumnTransformer([
    ("num", num_pipeline, num_attr),
    ("cat", cat_pipeline, cat_attr)
])

X_train_prep = pipeline.fit_transform(X_train)
X_test_prep = pipeline.transform(X_test)

# -------------------------------
# MODEL
# -------------------------------
model = RandomForestRegressor(
    n_estimators=args.n_estimators,
    max_depth=args.max_depth,
    min_samples_split=args.min_samples_split,
    random_state=42
)

model.fit(X_train_prep, y_train)

preds = model.predict(X_test_prep)
rmse = np.sqrt(mean_squared_error(y_test, preds))

# -------------------------------
# LOGGING
# -------------------------------
mlflow.log_params(vars(args))
mlflow.log_metric("rmse", rmse)

wandb.log({"rmse": rmse})

joblib.dump(model, "model.pkl")
joblib.dump(pipeline, "pipeline.pkl")

mlflow.log_artifact("model.pkl")
mlflow.log_artifact("pipeline.pkl")

wandb.save("model.pkl")
wandb.save("pipeline.pkl")

mlflow.end_run()
wandb.finish()

print(f"RMSE: {rmse}")
Save this as train.py. It shall be used by hyperparameter tuning

hyperparameter_tuner.py:
import sagemaker
from sagemaker.sklearn.estimator import SKLearn
from sagemaker.tuner import HyperparameterTuner, IntegerParameter

role = sagemaker.get_execution_role()
session = sagemaker.Session()

estimator = SKLearn(
    entry_point="train.py",
    role=role,
    instance_type="ml.m5.large",
    framework_version="1.2-1",
    py_version="py3",
    base_job_name="housing-mlops",
    sagemaker_session=session
)

hyperparameter_ranges = {
    "n_estimators": IntegerParameter(50, 300),
    "max_depth": IntegerParameter(5, 30),
    "min_samples_split": IntegerParameter(2, 10)
}

tuner = HyperparameterTuner(
    estimator=estimator,
    objective_metric_name="rmse",
    hyperparameter_ranges=hyperparameter_ranges,
    objective_type="Minimize",
    max_jobs=10,
    max_parallel_jobs=2
)

tuner.fit()

